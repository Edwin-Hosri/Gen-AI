# -*- coding: utf-8 -*-
"""Q&A Chatbot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hNoOTcZ90J8DRlXvfIX2eoh9cDTUF_1R
"""

!pip install langchain-huggingface

"""Installing Dependencies"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferWindowMemory
from langchain_huggingface import HuggingFacePipeline

"""HuggingFace Login Setup"""

from google.colab import userdata
userdata.get('HUGGING_FACE_HUB_TOKEN')

"""Program Code"""

model_name="meta-llama/Llama-3.2-1B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

device =0 if torch.cuda.is_available() else -1
transformer_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    temperature=0.5,
    top_p=0.8,
    top_k=50,
    device=device,
    max_new_tokens=150,  # Generates up to 150 tokens beyond the input(150 output tokens)
    repetition_penalty=1.5,
)

hf_pipeline= HuggingFacePipeline(pipeline=transformer_pipeline)

template = (
    "You are an AI assistant. Provide brief and factual answers.\n\n"
    "< Here some context that could be relevant to your AI answer: {history} >\n\n"
    "User: {input}\n"
    "AI:"
)

prompt=PromptTemplate(input_variables=["history","input"], template=template)

memory= ConversationBufferWindowMemory(k=3)

convo_chain= LLMChain(
                        prompt=prompt,
                        llm=hf_pipeline,
                        memory=memory,
                               )

if __name__ == "__main__":
  while True:
    try:
            question = input('Enter your question: ')
            output = convo_chain.run(input=question)
            answer_start = output.rfind("AI:") + len("AI:")  # Find the last instance of "AI:"
            answer = output[answer_start:].strip()  # Extract and clean the actual answer
            print(f'Answer: {answer}')
    except Exception as e:
            print(f'Error: {e}')
            break
memory.clear()