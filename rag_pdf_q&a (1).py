# -*- coding: utf-8 -*-
"""RAG_Pdf_Q&A

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zbvbdbs1ZaLUCi_UrpQwpJrG-FAMLWsQ
"""

!pip install langchain-huggingface
!pip install langchain_community
!pip install pypdf2
!pip install faiss-cpu
!pip install pypdf

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from langchain.prompts import PromptTemplate
from langchain.chains.combine_documents.stuff import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_huggingface import HuggingFacePipeline
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter #CharacterTextSplitter is used for no coherence and context is needed when chunking unlike RecursiveCTS
from langchain.document_loaders import PyPDFLoader #loading pdf loader from langchain and not directly
from langchain.vectorstores import FAISS #loading FAISS from langchain and not directly
#from langchain.schema import Document

from huggingface_hub import login
login(token="YOUR_HUGGING_FACE_TOKEN")

"""Typical Workflow

Load the Document: Use the PyPDFLoader to load the PDF into a list of Document objects.

Split the Text: Use the RecursiveCharacterTextSplitter to break each document's text into smaller chunks.

Embed and Index: Embed the chunks and store them in a vector database like FAISS.
"""

def pdfLoader(path):
  #Returns a list of Document objects "documents", where each object is usually a page in the pdf
  loader = PyPDFLoader(path)
  documents = loader.load()
  return documents

#after loading the pdf into the array, we need to further divide the data into chunks
def textSplitter(documents):
  #when considering chunk properties, it is primarly good idea to base the size of chunk size on model limit and the overlap on chunk size.
  #     chunk size is 75% of model token limit
  #     chunk overlap 15% of chunk size
  # to further improve chunk size selection we could also add the document size condition
  # since model token limit  in this case(llama 3.2 1b) is 128 000 the it is impractical to take 75% of it for chunk size whihc is 96 000 (size = (128_000*75)//100)
  # instaed we can opt for a good range of chunk size between 1000 - 4000 to keep good performance and coherence
  size = 3000
  overlap = (size*15)//100
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=overlap)
  docs = text_splitter.split_documents(documents)
  return docs

#Embed the chunks using HFembeddings and store them in the FAISS vectore db
def createMemory(docs):
  embeddings = HuggingFaceEmbeddings()
  db =FAISS.from_documents(docs, embeddings)
  return db

#initialize llm and return it
def initialize_llm():
  tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B")
  model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B")

  device = 0 if torch.cuda.is_available() else -1
  pipe = pipeline("text-generation",
                  tokenizer=tokenizer,
                  model=model,
                  temperature= 0.5,
                  top_p=0.8,
                  device=device,
                  )
  llm=HuggingFacePipeline(pipeline=pipe)
  return llm

"""You need to use create_stuff_documents_chain (or another chain) with RetrievalQA when you want to combine multiple retrieved documents into one coherent context before passing it to the model to answer the question.

In simpler terms:

-Use create_stuff_documents_chain when you have multiple retrieved documents and need to combine them into a single context for the model.

-Don't need it if the retriever gives you just one document or if you're okay with the model handling the documents directly without any combination.

Basically, it's required when you want to process multiple documents together before the model answers.
"""

if __name__ == "__main__":
    try:
        pdf_path = input("Enter the path of the PDF: ")
        documents = pdfLoader(pdf_path)
        if not documents:
            raise ValueError("No documents found in the PDF. Please check the file.")

        docs = textSplitter(documents)
        if not docs:
            raise ValueError("Document splitting resulted in no chunks. Check your splitter settings.")

        vector_db = createMemory(docs)
        retriever = vector_db.as_retriever(search_type="mmr")
        if retriever is None:
            raise ValueError("Retriever initialization failed. Ensure FAISS is set up correctly.")

        prompt_template = PromptTemplate(
            input_variables=["context", "input"],
            template="""
                You are a helpful and knowledgeable assistant.
                Use the context below to answer the question as concisely as possible.

                Context: {context}
                Question: {input}

                Answer:
            """
        )

        # Create the RetrievalQA chain
        llm = initialize_llm()
        question_answer_chain = create_stuff_documents_chain(llm, prompt_template)
        qa_chain = create_retrieval_chain(retriever, question_answer_chain)

        while True:
            question = input("Enter your question (or type 'exit' to quit): ")
            if question.lower() == "exit":
                print("Exiting the chatbot. Goodbye!")
                break
            try:
                answer = qa_chain.invoke({"input": question})
                print(f"Answer: {answer}")
            except Exception as e:
                print(f"Error during question processing: {e}")

    except Exception as e:
        print(f"Initialization error: {e}")

    finally:
        torch.cuda.empty_cache()
